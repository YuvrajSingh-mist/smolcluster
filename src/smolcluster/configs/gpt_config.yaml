# GPT Training Configuration
# Reproduces arXiv 2512.19428 exactly for Wikitext-2

# Model Architecture

vocab_size: null  # Will be set from tokenizer
model_dim: 256    # Model dimension 
num_layers: 6     # Number of layers (paper: 6 or 12)
num_heads: 4      # Number of attention heads (paper: 4)
ff_dim: 768      # Feed-forward dimension (paper: 1024)
dropout: 0.1      # Dropout rate
max_seq_len: 256  # Maximum sequence length (paper: 128 or 256)

# Training Hyperparameters
    
batch_size: 16             # Batch size
num_epochs: 2              # Number of epochs
learning_rate: 0.0003       # Learning rate
weight_decay: 0.01         # Weight decay for AdamW
grad_clip_norm: 1.0        # Gradient clipping norm
eval_steps: 200             # Evaluation steps interval
use_fp16: true            # Enable mixed precision training (fp16) with PyTorch AMP

# Learning Rate Schedule

use_lr_scheduler: true     # Enable/disable learning rate scheduling
warmup_iters: 300          # Linear warmup iterations
min_lr: 0.00003              # Minimum learning rate for cosine decay

# Data

dataset_name: "wikitext"              # Dataset name
dataset_config: "wikitext-103-v1"     # Dataset configuration/subset
tokenizer: "openai-community/gpt2"    # HuggingFace tokenizer

# Logging & Checkpoints

project_name: "smolcluster-gpt-wikitext2"
log_interval: 50           # Steps between logging
track_gradients: true      # Whether to track gradient norms

# Decoder-specific metrics
decoder_type:
  ppl: true                # If true, log Perplexity (PPL) metrics

# Checkpoint Settings
save_checkpoints: true     # Whether to save model checkpoints
checkpoint_dir: "checkpoints"  # Directory to save checkpoints (relative to project root)
checkpoint_steps: 500      # Save checkpoint every N steps (set to 0 to disable step-based saving)
