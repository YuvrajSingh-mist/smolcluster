# GPT Training Configuration
# Reproduces arXiv 2512.19428 exactly for Wikitext-2

# Model Architecture

vocab_size: null  # Will be set from tokenizer (GPT2)
model_dim: 256    # Model dimension (paper: 256)
num_layers: 6     # Number of layers (paper: 6 or 12)
num_heads: 4      # Number of attention heads (paper: 4)
ff_dim: 1024      # Feed-forward dimension (paper: 1024)
dropout: 0.1      # Dropout rate
max_seq_len: 128  # Maximum sequence length (paper: 128 or 256)

# Training Hyperparameters
task: "wikitext"           # Task to train on ["wikitext", "snli"]
batch_size: 16           # Batch size
num_epochs: 5               # Number of epochs
learning_rate: 0.0006        # Learning rate
weight_decay: 0.01         # Weight decay for AdamW
grad_clip_norm: 1.0        # Gradient clipping norm
eval_steps: 200             # Evaluation steps interval

# Learning Rate Schedule

use_lr_scheduler: true     # Enable/disable learning rate scheduling
warmup_iters: 400          # Linear warmup iterations
min_lr: 0.00006              # Minimum learning rate for cosine decay

# Data

dataset: "wikitext2"       # Dataset name
tokenizer: "openai-community/gpt2"  # HuggingFace tokenizer

# Logging & Checkpoints

project_name: "smolcluster-gpt-wikitext2"
log_interval: 50           # Steps between logging
track_gradients: true      # Whether to track gradient norms

# Decoder-specific metrics
decoder_type:
  ppl: true                # If true, log Perplexity (PPL) metrics

# Checkpoint Settings
save_checkpoints: true     # Whether to save model checkpoints
checkpoint_dir: "checkpoints"  # Directory to save checkpoints (relative to project root)
checkpoint_steps: 500      # Save checkpoint every N steps (set to 0 to disable step-based saving)
checkpoint_epochs: 1       # Save checkpoint every N epochs (set to 0 to disable epoch-based saving)
keep_last_n_checkpoints: 3 # Number of recent checkpoints to keep (0 = keep all)